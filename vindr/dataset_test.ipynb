{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.6.6-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.6.6\n"
     ]
    }
   ],
   "source": [
    "# !pip install pydicom\n",
    "# !pip install pylibjpeg\n",
    "# !pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import pydicom\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "import pylibjpeg\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class FaceLandmarksDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations/labels.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.image_labels = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.image_labels.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        image_file = pydicom.dcmread(os.path.join(self.root_dir, f'{self.image_labels.iloc[idx, 0]}.dicom'))\n",
    "        # plt.imshow(image_file.pixel_array, cmap=plt.cm.bone)\n",
    "        image = image_file.pixel_array\n",
    "        image = image / 4096                           # scaling from 0 to 1 range\n",
    "        image = (image * 256 // 1).astype(np.uint8)    # converting to uint8 and scaling to 0-255\n",
    "        label = self.image_labels.iloc[idx, 2:].astype(float)\n",
    "        label = torch.from_numpy(np.array(label))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        # print(image.shape, label.shape)\n",
    "        # print(type(image), type(label))\n",
    "        return image, label\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms = transforms.Compose([ transforms.ToPILImage(), transforms.Resize(size=(224, 224)), transforms.ToTensor() ])\n",
    "# test_transforms = transforms.Compose([ transforms.ToTensor() ])\n",
    "# test_transforms=None\n",
    "# test_transforms = transforms.Compose([ transforms.Resize(size=(224, 224)) ])\n",
    "\n",
    "\n",
    "train_labels_path = r\"image_labels_train.csv\"\n",
    "train_images_path = r\"train\"\n",
    "train_ds = FaceLandmarksDataset(train_labels_path, train_images_path, transform=test_transforms)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds,\n",
    "                                            batch_size=1, shuffle=True)\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    print(images.shape, labels.shape)\n",
    "    for image, label in zip(images, labels):\n",
    "        img = transforms.ToPILImage()(image)\n",
    "        # img.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 224, 224]) torch.Size([4, 15])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(images.shape, labels.shape)\n",
    "    for image, label in zip(images, labels):\n",
    "        img = transforms.ToPILImage()(image) #\n",
    "        img.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dims = 224, 224\n",
    "num_channels = 1\n",
    "learning_rate = 1e-3\n",
    "n_epochs = 2\n",
    "batch_size = 32\n",
    "\n",
    "num_classes = 15\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defining dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms = transforms.Compose([ transforms.ToPILImage(), transforms.Resize(size=(224, 224)), transforms.ToTensor() ])\n",
    "\n",
    "\n",
    "train_labels_path = r\"image_labels_train.csv\"\n",
    "train_images_path = r\"train\"\n",
    "train_ds = FaceLandmarksDataset(train_labels_path, train_images_path, transform=test_transforms)\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_labels_path = r\"image_labels_test.csv\"\n",
    "test_images_path = r\"test\"\n",
    "test_ds = FaceLandmarksDataset(test_labels_path, test_images_path, transform=test_transforms)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CNN                                      --                        --\n",
       "├─Sequential: 1-1                        [32, 15]                  --\n",
       "│    └─Conv2d: 2-1                       [32, 32, 224, 224]        320\n",
       "│    └─ReLU: 2-2                         [32, 32, 224, 224]        --\n",
       "│    └─Conv2d: 2-3                       [32, 32, 224, 224]        9,248\n",
       "│    └─ReLU: 2-4                         [32, 32, 224, 224]        --\n",
       "│    └─Conv2d: 2-5                       [32, 32, 224, 224]        9,248\n",
       "│    └─ReLU: 2-6                         [32, 32, 224, 224]        --\n",
       "│    └─AvgPool2d: 2-7                    [32, 32, 112, 112]        --\n",
       "│    └─Conv2d: 2-8                       [32, 64, 112, 112]        18,496\n",
       "│    └─ReLU: 2-9                         [32, 64, 112, 112]        --\n",
       "│    └─Conv2d: 2-10                      [32, 64, 112, 112]        36,928\n",
       "│    └─ReLU: 2-11                        [32, 64, 112, 112]        --\n",
       "│    └─Conv2d: 2-12                      [32, 64, 112, 112]        36,928\n",
       "│    └─ReLU: 2-13                        [32, 64, 112, 112]        --\n",
       "│    └─AvgPool2d: 2-14                   [32, 64, 56, 56]          --\n",
       "│    └─Conv2d: 2-15                      [32, 128, 56, 56]         73,856\n",
       "│    └─ReLU: 2-16                        [32, 128, 56, 56]         --\n",
       "│    └─AvgPool2d: 2-17                   [32, 128, 28, 28]         --\n",
       "│    └─Conv2d: 2-18                      [32, 256, 28, 28]         295,168\n",
       "│    └─ReLU: 2-19                        [32, 256, 28, 28]         --\n",
       "│    └─AvgPool2d: 2-20                   [32, 256, 14, 14]         --\n",
       "│    └─Conv2d: 2-21                      [32, 512, 14, 14]         1,180,160\n",
       "│    └─ReLU: 2-22                        [32, 512, 14, 14]         --\n",
       "│    └─AvgPool2d: 2-23                   [32, 512, 7, 7]           --\n",
       "│    └─Flatten: 2-24                     [32, 25088]               --\n",
       "│    └─Linear: 2-25                      [32, 1024]                25,691,136\n",
       "│    └─ReLU: 2-26                        [32, 1024]                --\n",
       "│    └─Linear: 2-27                      [32, 15]                  15,375\n",
       "│    └─Sigmoid: 2-28                     [32, 15]                  --\n",
       "==========================================================================================\n",
       "Total params: 27,366,863\n",
       "Trainable params: 27,366,863\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 90.32\n",
       "==========================================================================================\n",
       "Input size (MB): 6.42\n",
       "Forward/backward pass size (MB): 2029.78\n",
       "Params size (MB): 109.47\n",
       "Estimated Total Size (MB): 2145.67\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=num_channels, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels=32,           out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels=32,           out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            torch.nn.Conv2d(in_channels=32,           out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels=64,           out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels=64,           out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            torch.nn.Conv2d(in_channels=64,           out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            torch.nn.Conv2d(in_channels=128,           out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            torch.nn.Conv2d(in_channels=256,           out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # fully connected layers\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(7*7*512, 1024),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1024, num_classes),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = CNN().to(device)\n",
    "\n",
    "from torchinfo import summary\n",
    "summary(model, input_size=(batch_size, 1, *input_dims))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 15])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    # print shape of outputs of model on this batch\n",
    "    print(model(images).shape)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c163f7d3600bce78318f6aebf70f2fd9e9ada0d9e383f1dbf1d4b5484416c593"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
