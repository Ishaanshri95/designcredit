{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading festival image dataset from github"
      ],
      "metadata": {
        "id": "Y8NSycPmUfr_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCEGph0-szgC",
        "outputId": "d7629d85-f25e-4521-f48b-065064b7e159"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-13 13:51:18--  https://github.com/Ishaanshri95/datasets/archive/refs/heads/main.zip\n",
            "Resolving github.com (github.com)... 13.114.40.48\n",
            "Connecting to github.com (github.com)|13.114.40.48|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/Ishaanshri95/datasets/zip/refs/heads/main [following]\n",
            "--2022-02-13 13:51:18--  https://codeload.github.com/Ishaanshri95/datasets/zip/refs/heads/main\n",
            "Resolving codeload.github.com (codeload.github.com)... 13.112.159.149\n",
            "Connecting to codeload.github.com (codeload.github.com)|13.112.159.149|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘main.zip’\n",
            "\n",
            "main.zip                [            <=>     ]  51.97M  14.2MB/s    in 4.5s    \n",
            "\n",
            "2022-02-13 13:51:23 (11.6 MB/s) - ‘main.zip’ saved [54492797]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "url = \"https://github.com/Ishaanshri95/datasets/archive/refs/heads/main.zip\"\n",
        "\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    \"https://github.com/Ishaanshri95/datasets/archive/refs/heads/main.zip\"\n",
        "\n",
        "\n",
        "dir = 'tmp'\n",
        "if os.path.exists(dir):\n",
        "    shutil.rmtree(dir)\n",
        "os.makedirs(dir)\n",
        "\n",
        "zip_ref = zipfile.ZipFile('main.zip', 'r') #Opens the zip file in read mode\n",
        "zip_ref.extractall('tmp') #Extracts the files into the tmp folder\n",
        "zip_ref.close()\n",
        "os.remove('main.zip')\n",
        "shutil.move('/content/tmp/datasets-main/festivals', '/content/festivals')\n",
        "shutil.rmtree('tmp')\n",
        "shutil.rmtree('festivals/festivals')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some imports, setting the device"
      ],
      "metadata": {
        "id": "1SsWO4wYC_nT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "import numpy as np\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "KTVQZ3Ab85ew",
        "outputId": "e31a10de-6950-4703-9bdd-6e4c34bb9d39"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# festival_list_class = [i for i in range(len(festival_list))]\n",
        "# converter = dict(zip(festival_list, festival_list_class))\n",
        "# festival_data = []\n",
        "# festival_target = []\n",
        "\n",
        "# for fest in festival_list:\n",
        "#   print()\n",
        "#   print(fest)\n",
        "#   imgs = next(os.walk(root+'/'+fest))[2]\n",
        "\n",
        "#   for img in imgs:\n",
        "#     img_data = cv2.imread(root+'/'+fest+'/'+img)\n",
        "#     img_data = cv2.cvtColor(img_data, cv2.COLOR_BGR2RGB)\n",
        "#     img_data = cv2.resize(img_data, (100,100), interpolation=cv2.INTER_LINEAR)\n",
        "#     # print(img_data.shape)\n",
        "\n",
        "#     festival_data.append(img_data)\n",
        "#     targ = [0 for i in festival_list]\n",
        "#     targ[converter[fest]] = 1\n",
        "#     festival_target.append(targ)\n",
        "\n",
        "# festival_data = np.array(festival_data)\n",
        "# festival_target = np.array(festival_target)\n",
        "# festival_data = torch.tensor(festival_data).to(device)\n",
        "# festival_target = torch.tensor(festival_target).to(device)\n",
        "# # festival_data = np.array(festival_data)                    # throws deprecation warning"
      ],
      "metadata": {
        "id": "KlAl4PGEHO5G"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the dataset, making batch loaders"
      ],
      "metadata": {
        "id": "Eedegg8hwIBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "\n",
        "root = '/content/festivals'\n",
        "\n",
        "def get_data_loader(root=root, batch_size = 32, train=True):\n",
        "  transform = {\n",
        "    'train': transforms.Compose([\n",
        "      transforms.Resize([100,100]), # Resizing the image as the VGG only take 224 x 244 as input size\n",
        "      #TODO if it is needed, add the random crop\n",
        "      transforms.ToTensor(),\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "      transforms.Resize([100,100]),\n",
        "      transforms.ToTensor()\n",
        "    ])\n",
        "  }\n",
        "  tt = lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1)\n",
        "  # data = torchvision.datasets.ImageFolder(root=root, transform=transform['train'] if train else 'test', target_transform=tt)\n",
        "  data = torchvision.datasets.ImageFolder(root=root, transform=transform['train'] if train else transform['test'])\n",
        "  data_loader = DataLoader(dataset=data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "  \n",
        "  # print(data)\n",
        "  return data_loader\n",
        "\n",
        "\n",
        "torch.multiprocessing.freeze_support()\n",
        "train_loader = get_data_loader()\n",
        "test_loader = get_data_loader(train=False)\n",
        "\n",
        "# print(type(test_loader))\n",
        "for i in range(10):\n",
        "  batch_x, batch_y = next(iter(test_loader))\n",
        "  print(np.shape(batch_x), batch_y.shape) \n",
        "\n",
        "root = 'festivals'\n",
        "# # shutil.rmtree('tmp')\n",
        "festival_list = next(os.walk(root))[1]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqJa3Cm1tCJk",
        "outputId": "52ff7e8f-a873-4471-8956-31a31ce295ad"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3, 100, 100]) torch.Size([32])\n",
            "torch.Size([32, 3, 100, 100]) torch.Size([32])\n",
            "torch.Size([32, 3, 100, 100]) torch.Size([32])\n",
            "torch.Size([32, 3, 100, 100]) torch.Size([32])\n",
            "torch.Size([32, 3, 100, 100]) torch.Size([32])\n",
            "torch.Size([32, 3, 100, 100]) torch.Size([32])\n",
            "torch.Size([32, 3, 100, 100]) torch.Size([32])\n",
            "torch.Size([32, 3, 100, 100]) torch.Size([32])\n",
            "torch.Size([32, 3, 100, 100]) torch.Size([32])\n",
            "torch.Size([32, 3, 100, 100]) torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining Hyperparameters"
      ],
      "metadata": {
        "id": "cDkJhrprCswe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(festival_list)\n",
        "batch_size = 16                        # besties don't let each other use batches larger than 32 :)\n",
        "learning_rate = 0.001\n",
        "num_epochs = 40"
      ],
      "metadata": {
        "id": "ZxybkJIMK-8z"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining CNN class\n",
        "Note: model takes in input layer of size 100x100."
      ],
      "metadata": {
        "id": "SSsaZai_CfTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super(CNN, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    self.pool = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
        "    self.conv2 = nn.Conv2d(in_channels = 24, out_channels = 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    self.conv3 = nn.Conv2d(in_channels = 48, out_channels = 96, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
        "    self.conv4 = nn.Conv2d(in_channels = 96, out_channels = 192, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
        "    self.fc1 = nn.Linear(192*5*5, 400)\n",
        "    self.fc2 = nn.Linear(400, num_classes)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.pool(self.conv1(x))           # convolutional layer backbone\n",
        "    x = self.pool(self.conv2(x))           # convolutional layer backbone\n",
        "    x = self.pool(self.conv3(x))           # convolutional layer backbone\n",
        "    x = self.pool(self.conv4(x))           # convolutional layer backbone\n",
        "    x = x.reshape(x.shape[0], -1)        # Fully connected neural network layers\n",
        "    x = F.relu(self.fc1(x))              # Fully connected neural network layers\n",
        "    x = self.fc2(x)                      # Fully connected neural network layers\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "Cwy9FUOWlF1w"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the CNN model"
      ],
      "metadata": {
        "id": "QOMq-n0jCUYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(64, 3, 100, 100)                    # 100 x 100 images with 3 RGB channels\n",
        "x = x.to(device)                                   # loads data onto gpu if available\n",
        "model = CNN(num_classes=num_classes).to(device)    # creates model\n",
        "model(x).shape                                     # checks if layer sizes are consistent with input and target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evLOey_ZtW09",
        "outputId": "c3f3fd15-356c-4155-f898-337864fe12f7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss and Optimizer"
      ],
      "metadata": {
        "id": "AJF0Si7sDUaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "4lFL7HaGDKLY"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Network"
      ],
      "metadata": {
        "id": "xOO5ikoRDwsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_accuracy(loader, model):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:                    # takes one batch from the data loader\n",
        "            x = x.to(device=device)\n",
        "            y = y.to(device=device)\n",
        "\n",
        "            scores = model(x)\n",
        "            _, predictions = scores.max(1)     # finds which index of the predicted target is maximum    [num_samples]\n",
        "            num_correct += (predictions == y).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "    model.train()\n",
        "    return num_correct/num_samples\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  # print(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:.2f}\")\n",
        "  for batch_idx, (data, targets) in enumerate(tqdm(train_loader)):\n",
        "    data = data.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    data = data.reshape(data.shape[0], 3, 100, 100)\n",
        "\n",
        "    scores = model(data)\n",
        "    loss = criterion(scores, targets)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lNVJLpxD4Kl",
        "outputId": "f4a328fb-71ba-4c2a-d72f-11bd635267e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 133/133 [00:07<00:00, 17.59it/s]\n",
            "100%|██████████| 133/133 [00:07<00:00, 17.72it/s]\n",
            "100%|██████████| 133/133 [00:07<00:00, 18.00it/s]\n",
            "100%|██████████| 133/133 [00:07<00:00, 17.74it/s]\n",
            "100%|██████████| 133/133 [00:07<00:00, 17.67it/s]\n",
            "100%|██████████| 133/133 [00:07<00:00, 17.37it/s]\n",
            "100%|██████████| 133/133 [00:07<00:00, 17.44it/s]\n",
            "100%|██████████| 133/133 [00:07<00:00, 17.47it/s]\n",
            "100%|██████████| 133/133 [00:07<00:00, 17.64it/s]\n",
            "100%|██████████| 133/133 [00:07<00:00, 17.64it/s]\n",
            "100%|██████████| 133/133 [00:07<00:00, 17.33it/s]\n",
            "100%|██████████| 133/133 [00:07<00:00, 17.47it/s]\n",
            "100%|██████████| 133/133 [00:07<00:00, 17.85it/s]\n",
            "100%|██████████| 133/133 [00:07<00:00, 17.34it/s]\n",
            "100%|██████████| 133/133 [00:07<00:00, 17.52it/s]\n",
            "100%|██████████| 133/133 [00:07<00:00, 17.74it/s]\n",
            "100%|██████████| 133/133 [00:07<00:00, 17.40it/s]\n",
            "100%|██████████| 133/133 [00:07<00:00, 17.65it/s]\n",
            "  1%|          | 1/133 [00:00<00:30,  4.40it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate model accuracy on train and test"
      ],
      "metadata": {
        "id": "zuI7CgEpE2UI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_accuracy(loader, model):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device=device)\n",
        "            y = y.to(device=device)\n",
        "            # x = x.reshape(x.shape[0], -1)    # dont include this line in cnn\n",
        "\n",
        "            scores = model(x)\n",
        "            _, predictions = scores.max(1)\n",
        "            # print(num_correct.shape)\n",
        "\n",
        "            # print(predictions.shape)\n",
        "            # print(scores.shape)\n",
        "            # print(y.shape)\n",
        "            num_correct += (predictions == y).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "    model.train()\n",
        "    return num_correct/num_samples\n",
        "\n",
        "print(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:.2f}\")\n",
        "print(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4Gs-sTUE11Z",
        "outputId": "6478d557-3d09-4241-c9d1-0a98c91f6ca8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on training set: 98.89\n",
            "Accuracy on test set: 98.89\n"
          ]
        }
      ]
    }
  ]
}